# -*- coding: utf-8 -*-
"""myVGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1loCK0-qi6mOoyw2qI0libHIT79YM_bSH
"""

#drive conection
import os
from google.colab import drive
drive.mount('/gdrive')

#importing dataset from drive and unzip
!cp /gdrive/My\ Drive/pibic/RIMONE-db-r2.tar.gz .
!tar -xvf RIMONE-db-r2.tar.gz
#!rm -rf RIMONE-db-r2
#!cp /gdrive/My\ Drive/pibic/RIMONE-db-r2.zip .
#!unzip -q RIMONE-db-r2.zip

#removing unecessary documents from dataset
!rm -rf RIMONE-db-r2/1/*.txt
!rm -rf RIMONE-db-r2/1/*.bmp
!rm -rf RIMONE-db-r2/0/*.txt
!rm -rf RIMONE-db-r2/0/*.bmp

#organize dataset folders
!mkdir RIMONE-db-r2/treino
!mkdir RIMONE-db-r2/treino/true
!mkdir RIMONE-db-r2/treino/false

!mv RIMONE-db-r2/0/* RIMONE-db-r2/treino/false
!mv RIMONE-db-r2/1/* RIMONE-db-r2/treino/true


#gera uma base de validação baseado no treino (20%)
!mkdir RIMONE-db-r2/test
!mkdir RIMONE-db-r2/test/true
!mkdir RIMONE-db-r2/test/false
!mkdir RIMONE-db-r2/valid
!mkdir RIMONE-db-r2/valid/true
!mkdir RIMONE-db-r2/valid/false

#conect drive to colab 

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

#creates drive directory connect to google drive
!mkdir -p drive
!google-drive-ocamlfuse drive

#move files to validation folder
import os
import shutil
import random

def get_file_list(input_dir):
    return [file for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]

def get_random_files(file_list, N):
    return random.sample(file_list, N)

def copy_files(random_files, input_dir, output_dir):
    for file in random_files:
        shutil.move(os.path.join(input_dir, file), output_dir)

def main(input_dir, output_dir, N):
    file_list = get_file_list(input_dir)
    random_files = get_random_files(file_list, N)
    copy_files(random_files, input_dir, output_dir)
    
main('RIMONE-db-r2/treino/true', 'RIMONE-db-r2/test/true/', 40) #20%
main('RIMONE-db-r2/treino/false/', 'RIMONE-db-r2/test/false/', 50)
main('RIMONE-db-r2/treino/true/', 'RIMONE-db-r2/valid/true/', 20) #10%
main('RIMONE-db-r2/treino/false/', 'RIMONE-db-r2/valid/false/', 25)

#IMPORTS

from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization
from keras.models import Sequential, load_model, Model
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from keras.optimizers import Adam

from keras.applications.vgg16 import VGG16 
from keras.applications.vgg19 import VGG19
from keras.applications.xception import Xception
from keras.applications.densenet import DenseNet169
from keras.applications.densenet import DenseNet121
from keras.applications.densenet import DenseNet201
from keras.applications.resnet50 import ResNet50
from keras.applications.inception_v3 import InceptionV3
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications.nasnet import NASNetMobile 

from keras.applications.vgg16 import preprocess_input, decode_predictions
from keras.applications.xception import preprocess_input, decode_predictions
from keras.applications.densenet import preprocess_input, decode_predictions

from sklearn import metrics
from sklearn.model_selection import train_test_split
from glob import glob
import matplotlib.pyplot as plt
import numpy as np
import os

from datetime import datetime

#DATASET RELATED

# You need to have these three folders each with two subfolders for the two classes.
train_data_dir = "RIMONE-db-r2/treino"
validation_data_dir = "RIMONE-db-r2/valid"
tests_data_dir = "RIMONE-db-r2/test"

# 600/450 _ 500/375 _ 400/300 _ 300/225

img_width = 128  # Change image size for training here
img_height = 128 # Change image size for training here

batch_size = 5 # i achieved good and fast results with this small minibatch size for training
batch_size_val = 4 # if Tensorflow throws a memory error while validating at end of epoch, decrease validation batch size her


# set data augmentation parameters here
datagen = ImageDataGenerator(rescale=1., 
    featurewise_center=True,
    rotation_range=10,
    width_shift_range=.1,
    height_shift_range=.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=False,
    fill_mode="reflect")

# normalization neccessary for correct image input to VGG16
datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)

# no data augmentation for validation
# set data augmentation parameters here
validgen = ImageDataGenerator(rescale=1., 
    featurewise_center=True,
    rotation_range=10,
    width_shift_range=.1,
    height_shift_range=.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=False,
    fill_mode="reflect")

validgen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)


train_gen = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode="binary",
        shuffle=True, 
        #save_to_dir="_augmented_images/", 
        #save_prefix="aug_"
        )

val_gen = validgen.flow_from_directory(
        validation_data_dir,
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode="binary",
        shuffle=True)

# no data augmentation for test
teste = ImageDataGenerator(rescale=1., featurewise_center=True)
test_gen = teste.flow_from_directory(
        tests_data_dir,
        target_size=(img_height, img_width),
        batch_size=1,
        class_mode="binary",
        shuffle=False)

#amount of samples
train_samples = len(train_gen.filenames)
validation_samples = len(val_gen.filenames)
test_samples = len(test_gen.filenames)

#NN RELATED

vgg16 = VGG16(weights='imagenet', include_top=False)
#vgg19 = VGG19(weights='imagenet', include_top=False)

for layer in vgg16.layers:
  layer.trainable = False

#MODEL RELATED

model = Sequential()

def func(model, dropout):
  model.add(Dropout(dropout))
  return model


model.add(vgg16)

model.add(GlobalAveragePooling2D())

model.add(Dense(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid')) #Saida = 2


'''
#loading weights
#weight_path = 'drive/pibic/weights_best.h5'
#model.load_weights(weight_path)
'''

#CALLBACKS

#save weights
checkpoint = ModelCheckpoint('drive/pibic/weights_best.h5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)

#stop if finds a very good one
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')

'''
#reduce the learnig rate if validation loss is too big

#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
'''

#all my callbacks
callbacks_list = [checkpoint, early_stopping]

#TRAINING RELATED
#trainable = false 

#compile model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc', 'mae'])

#training
history = model.fit_generator(epochs=30, 
                              callbacks=callbacks_list, 
                              shuffle=True, 
                              validation_data=val_gen, 
                              generator=train_gen, 
                              steps_per_epoch=300, 
                              validation_steps=100)                              
#saving model
model.save("drive/pibic/VGG16.h5")

for layer in model.layers[:15]:
  layer.trainable = False

for layer in model.layers[15:]:
  layer.trainable = True

#TRAINING RELATED
#trainable true

#compile model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc', 'mae'])

#training
history = model.fit_generator(epochs=80, 
                              callbacks=callbacks_list, 
                              shuffle=True, 
                              validation_data=val_gen, 
                              generator=train_gen, 
                              steps_per_epoch=300, 
                              validation_steps=100)                              

#saving model
model.save("drive/pibic/VGG16.h5")

preds = model.predict_generator(test_gen, len(test_gen))
preds_rounded = []

for pred in preds:
    if (pred > .5):
        preds_rounded.append("1")
    else:
        preds_rounded.append("0")

file_list = get_file_list(tests_data_dir + '/false')

tp = 0
fp = 0
tn = 0
fn = 0

for i in range(0, len(file_list)):
  if (preds_rounded[i] == '0'):
    tn=tn+1
  else:
    fp=fp+1

ini = len(file_list)
file_list = get_file_list(tests_data_dir + '/true')  
for i in range(ini, ini+len(file_list)-1):
  if (preds_rounded[i] == '1'):
    tp=tp+1
  else:
    fn=fn+1


S = tp/(tp+fn)#sensibility
P = tp/(tp+fp)#precision
E = tn/(tn+fp)#specifity
A = (tp+tn)/(tp+tn+fp+fn)#accuracy

hs = open("log.txt","a")
hs.write("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(tn, fp, fn, tp, P, S, E, A))
print('A: ' + str(A) + '\tS: ' + str(S) + '\tP: ' + str(P) + '\tE: ' + str(E)  )